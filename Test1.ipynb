{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "25\n",
      "36\n",
      "927092\n",
      "234100\n",
      "[[5.89973200e-03 5.94546611e-04 9.11943056e-03 1.78363994e-02\n",
      "  2.05804594e-03 9.66824312e-03 2.40059644e-01 6.40463904e-02\n",
      "  6.26039326e-01 1.02353487e-02 1.45435252e-03 1.29885571e-02]\n",
      " [1.51888773e-01 2.29538302e-04 7.23373592e-02 9.19792727e-02\n",
      "  2.51836311e-02 1.89533047e-02 1.11326076e-01 1.95861757e-01\n",
      "  2.43933633e-01 6.74514696e-02 5.90241340e-04 2.02649534e-02]\n",
      " [4.55752701e-01 1.90329528e-03 2.06074975e-02 1.52523164e-02\n",
      "  1.45256035e-02 3.02883498e-02 8.21877494e-02 4.13880199e-02\n",
      "  2.58346379e-01 6.95567951e-02 4.23915772e-04 9.76736564e-03]\n",
      " [7.33235255e-02 1.70099884e-02 1.40716955e-01 9.66648832e-02\n",
      "  2.88701616e-02 1.35321915e-02 1.37618154e-01 2.41394684e-01\n",
      "  3.25708948e-02 4.77973968e-02 1.11467809e-04 1.70389697e-01]\n",
      " [8.41507018e-02 1.20816668e-02 9.00862962e-02 3.52346860e-02\n",
      "  1.06082931e-02 5.34624280e-03 1.91538632e-02 6.21721745e-01\n",
      "  3.64975780e-02 6.86171325e-03 8.41928049e-05 7.81730190e-02]\n",
      " [1.45397140e-02 3.94649394e-02 1.30358920e-01 2.13526096e-02\n",
      "  5.40046534e-03 2.21834499e-02 5.97374551e-02 4.58624139e-02\n",
      "  3.78032565e-01 9.47158504e-03 2.49252247e-04 2.73346633e-01]\n",
      " [5.96689060e-03 3.77903059e-02 8.77985284e-02 9.67563409e-03\n",
      "  1.95157696e-02 7.25298701e-03 5.64387068e-02 1.76464431e-02\n",
      "  6.52843595e-01 3.81342624e-03 5.08456840e-04 1.00749224e-01]\n",
      " [1.62716493e-01 1.46433180e-02 1.70271456e-01 1.03463896e-01\n",
      "  6.52156025e-02 8.96165520e-03 5.73998839e-02 1.84015870e-01\n",
      "  9.78234038e-02 5.47100157e-02 1.98995418e-04 8.05794224e-02]\n",
      " [1.61504224e-02 5.98919056e-02 2.44804248e-01 2.67281309e-02\n",
      "  1.79171581e-02 8.25233944e-03 1.31710423e-02 1.58370420e-01\n",
      "  1.50458708e-01 2.01789439e-02 3.63339088e-04 2.83713311e-01]\n",
      " [1.73528064e-02 1.13225793e-02 5.53152524e-02 5.35087287e-02\n",
      "  2.37646941e-02 1.04320387e-03 9.82138328e-03 7.07928360e-01\n",
      "  9.08350758e-03 8.42196308e-03 2.54439983e-05 1.02412090e-01]\n",
      " [5.52486209e-03 2.20994484e-02 5.52486181e-02 9.20810271e-03\n",
      "  8.28729291e-03 9.20810329e-04 2.76243105e-03 5.80110513e-02\n",
      "  5.61694279e-02 8.28729291e-03 4.98158365e-01 2.75322288e-01]\n",
      " [1.48900434e-01 8.79148841e-02 1.11679561e-01 7.79287964e-02\n",
      "  3.20199542e-02 1.90813132e-02 4.20314930e-02 9.39302891e-02\n",
      "  1.36606604e-01 1.07361026e-01 1.39991858e-03 1.41137242e-01]]\n",
      "           DET      CONJ       ADP       ADV       PRT       NUM       ADJ  \\\n",
      "DET   0.005900  0.000595  0.009119  0.017836  0.002058  0.009668  0.240060   \n",
      "CONJ  0.151889  0.000230  0.072337  0.091979  0.025184  0.018953  0.111326   \n",
      "ADP   0.455753  0.001903  0.020607  0.015252  0.014526  0.030288  0.082188   \n",
      "ADV   0.073324  0.017010  0.140717  0.096665  0.028870  0.013532  0.137618   \n",
      "PRT   0.084151  0.012082  0.090086  0.035235  0.010608  0.005346  0.019154   \n",
      "NUM   0.014540  0.039465  0.130359  0.021353  0.005400  0.022183  0.059737   \n",
      "ADJ   0.005967  0.037790  0.087799  0.009676  0.019516  0.007253  0.056439   \n",
      "VERB  0.162716  0.014643  0.170271  0.103464  0.065216  0.008962  0.057400   \n",
      "NOUN  0.016150  0.059892  0.244804  0.026728  0.017917  0.008252  0.013171   \n",
      "PRON  0.017353  0.011323  0.055315  0.053509  0.023765  0.001043  0.009821   \n",
      "X     0.005525  0.022099  0.055249  0.009208  0.008287  0.000921  0.002762   \n",
      ".     0.148900  0.087915  0.111680  0.077929  0.032020  0.019081  0.042031   \n",
      "\n",
      "          VERB      NOUN      PRON         X         .  \n",
      "DET   0.064046  0.626039  0.010235  0.001454  0.012989  \n",
      "CONJ  0.195862  0.243934  0.067451  0.000590  0.020265  \n",
      "ADP   0.041388  0.258346  0.069557  0.000424  0.009767  \n",
      "ADV   0.241395  0.032571  0.047797  0.000111  0.170390  \n",
      "PRT   0.621722  0.036498  0.006862  0.000084  0.078173  \n",
      "NUM   0.045862  0.378033  0.009472  0.000249  0.273347  \n",
      "ADJ   0.017646  0.652844  0.003813  0.000508  0.100749  \n",
      "VERB  0.184016  0.097823  0.054710  0.000199  0.080579  \n",
      "NOUN  0.158370  0.150459  0.020179  0.000363  0.283713  \n",
      "PRON  0.707928  0.009084  0.008422  0.000025  0.102412  \n",
      "X     0.058011  0.056169  0.008287  0.498158  0.275322  \n",
      ".     0.093930  0.136607  0.107361  0.001400  0.141137  \n",
      "///////////////////////////////////////////////////////////////////////////\n",
      "['ADV', '.', 'DET', 'X', 'NOUN', 'VERB', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'ADJ', 'NOUN', 'DET', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NUM', '.', 'PRT', 'VERB', 'ADV', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "///////////////////////////////////////////////////////////////////////////\n",
      "['ADV', '.', 'DET', 'DET', 'NOUN', 'VERB', 'ADV', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'DET', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NUM', '.', 'PRT', 'VERB', 'ADV', 'DET', 'NOUN', 'ADP', 'DET', 'DET', 'NOUN', 'ADP', 'NOUN', '.']\n",
      "[[3 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 0 0 1 0 0 0 0]\n",
      " [0 0 6 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 6 0 0 0 0 0]\n",
      " [0 0 0 0 1 6 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 4 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n",
      "[('However', 'ADV'), (',', '.'), ('the', 'DET'), ('wei', 'X'), ('books', 'NOUN'), ('were', 'VERB'), ('also', 'ADV'), ('destroyed', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('series', 'NOUN'), ('of', 'ADP'), ('Orthodox', 'ADJ'), ('Confucian', 'ADJ'), ('purges', 'NOUN'), ('which', 'DET'), ('culminated', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('final', 'ADJ'), ('proscription', 'NOUN'), ('in', 'ADP'), ('605', 'NUM'), ('.', '.'), ('There', 'PRT'), ('is', 'VERB'), ('also', 'ADV'), ('the', 'DET'), ('problem', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), (\"respondent's\", 'NOUN'), ('frame', 'NOUN'), ('of', 'ADP'), ('reference', 'NOUN'), ('.', '.')]\n",
      "///////////////////////////////////////////////////////////////////////////\n",
      "[('However', 'ADV'), (',', '.'), ('the', 'DET'), ('wei', 'DET'), ('books', 'NOUN'), ('were', 'VERB'), ('also', 'ADV'), ('destroyed', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('series', 'NOUN'), ('of', 'ADP'), ('Orthodox', 'ADJ'), ('Confucian', 'NOUN'), ('purges', 'VERB'), ('which', 'DET'), ('culminated', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('final', 'ADJ'), ('proscription', 'NOUN'), ('in', 'ADP'), ('605', 'NUM'), ('.', '.'), ('There', 'PRT'), ('is', 'VERB'), ('also', 'ADV'), ('the', 'DET'), ('problem', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), (\"respondent's\", 'DET'), ('frame', 'NOUN'), ('of', 'ADP'), ('reference', 'NOUN'), ('.', '.')]\n",
      "////////////////////////////////////////////////////////////////////\n",
      "///////////////////////////////////////////////////////////////////////////\n",
      "32\n",
      "36\n",
      "0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint,time\n",
    "\n",
    "brown_words=[]\n",
    "for sent in brown.tagged_sents(tagset='universal'):\n",
    "    brown_words.append(sent)\n",
    "test_untagged_words1=[tup[0] for sent in brown_words[:1] for tup in sent]\n",
    "print(test_untagged_words1)\n",
    "print(len(test_untagged_words1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_set,test_set=train_test_split(brown_words,train_size=0.80,test_size=0.20,random_state=101)\n",
    "\n",
    "\n",
    "\n",
    "test_untagged_words1=[tup[0] for sent in test_set[:2] for tup in sent]\n",
    "print(len(test_untagged_words1))\n",
    "\n",
    "train_tagged_words=[]\n",
    "for sent in train_set:\n",
    "    for tup in sent:\n",
    "        train_tagged_words.append(tup)\n",
    "test_tagged_words=[]\n",
    "for sent in test_set:\n",
    "    for tup in sent:\n",
    "        test_tagged_words.append(tup)\n",
    "print(len(train_tagged_words))\n",
    "print(len(test_tagged_words))\n",
    "\n",
    "tags={ tag for word,tag in train_tagged_words}\n",
    "\n",
    "\n",
    "\n",
    "def word_given_tag(word,tag,train_bag=train_tagged_words):\n",
    "    tag_list=[]\n",
    "    for pair in train_bag:\n",
    "        if pair[1]==tag:\n",
    "            tag_list.append(pair)\n",
    "    count_tag=len(tag_list)\n",
    "    w_given_tag_list=[]\n",
    "    for pair in tag_list:\n",
    "        if pair[0]==word:\n",
    "            w_given_tag_list.append(pair[0])\n",
    "    count_w_given_tag=len(w_given_tag_list)\n",
    "    return(count_w_given_tag,count_tag)\n",
    "\n",
    "\n",
    "\n",
    "def t2_given_t1(t2,t1,train_bag=train_tagged_words):\n",
    "    tags=[pair[1] for pair in train_bag]\n",
    "    count_t1=len([t for t in tags if t==t1])\n",
    "    count_t2_t1=0\n",
    "    for i in range(len(tags)-1):\n",
    "        if tags[i]==t1 and tags[i+1]==t2:\n",
    "            count_t2_t1+=1\n",
    "    return (count_t2_t1,count_t1)\n",
    "\n",
    "\n",
    "\n",
    "tags_matrix=np.zeros((len(tags),len(tags)),dtype='float32')\n",
    "for i,t1 in enumerate(list(tags)):\n",
    "    for j,t2 in enumerate(list(tags)):\n",
    "        tags_matrix[i,j]=t2_given_t1(t2,t1)[0]/t2_given_t1(t2,t1)[1]\n",
    "        \n",
    "print(tags_matrix)\n",
    "\n",
    "\n",
    "\n",
    "tags_df=pd.DataFrame(tags_matrix,columns=list(tags),index=list(tags))\n",
    "print(tags_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def Viterbi(words,train_bag=train_tagged_words):\n",
    "    state=[]\n",
    "    T=list(set([pair[1] for pair in train_bag]))\n",
    "    i=0\n",
    "    \n",
    "    for key,word in enumerate(words):\n",
    "        p=[]\n",
    "    \n",
    "        for tag in T:\n",
    "            if key==0:\n",
    "                transition_p=tags_df.loc['.',tag]\n",
    "            else:\n",
    "                transition_p=tags_df.loc[state[-1],tag]\n",
    "                \n",
    "                \n",
    "            emission_p=word_given_tag(words[key],tag)[0]/word_given_tag(words[key],tag)[1]\n",
    "            state_probability=emission_p*transition_p\n",
    "            p.append(state_probability)\n",
    "        \n",
    "        pmax=max(p)\n",
    "        state_max=T[p.index(pmax)]\n",
    "        state.append(state_max)\n",
    "        \n",
    "    return list(zip(words,state))\n",
    "\n",
    "\n",
    "test_tagged_words=[tup for sent in test_set[:2] for tup in sent]\n",
    "tagged_seq=Viterbi(test_untagged_words1)\n",
    "y_actual=[tup[1] for sent in test_set[:2] for tup in sent]\n",
    "y_pred=[tup[1] for tup in tagged_seq]\n",
    "print(\"///////////////////////////////////////////////////////////////////////////\")\n",
    "print(y_actual)\n",
    "print(\"///////////////////////////////////////////////////////////////////////////\")\n",
    "print(y_pred)\n",
    "\n",
    "c_matrix=confusion_matrix(y_actual,y_pred)\n",
    "print(c_matrix)\n",
    "tags_df=pd.DataFrame(c_matrix)\n",
    "tags_df\n",
    "\n",
    "check=[i for i , j in zip(tagged_seq,test_tagged_words) if i==j]\n",
    "\n",
    "acc=len(check)/len(tagged_seq)\n",
    "print(test_tagged_words)\n",
    "print(\"///////////////////////////////////////////////////////////////////////////\")\n",
    "print(tagged_seq)\n",
    "print(\"////////////////////////////////////////////////////////////////////\")\n",
    "print(\"///////////////////////////////////////////////////////////////////////////\")\n",
    "print(len(check))\n",
    "print(len(tagged_seq))\n",
    "print(acc)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
